---
title: 新芽专题介绍（18）：大规模论文学术索引和动态影响力评估数据集
date: 2025-09-17T01:33:00Z
draft: false
math: true
authors: 
- admin
---

## 一、专题介绍

### 1.1 研究背景

在当今科研信息高速增长的时代，学术论文数量呈指数级上升，科研人员面临着**如何在海量论文中高效、精准地检索到与自己研究主题高度相关的文献**这一核心挑战。传统基于关键词的检索方法已难以满足需求，逐渐被基于语义表示的论文检索与推荐系统所取代，这类系统通常采用两阶段检索架构：

- **召回阶段（Retrieval）**：使用嵌入（Embedding）模型快速从大规模论文库中召回一批候选论文；
- **精排阶段（Rerank）**：使用交互式模型（Cross-Encoder）对候选论文进行精细相关性评分，从而选出最优结果。

然而，**rerank模型虽然精度极高，但推理速度极慢，无法在大规模论文库上实时部署；embedding模型推理快速，但精度有限，难以保证推荐质量**。与此同时，reranker 通常可以使用更大规模的数据进行训练，因此其效果常显著优于 embedding 模型。

其核心思路是：**利用精度更高的reranker模型离线生成大规模伪标注数据，再通过Prompt Engineering（提示词工程）在embedding模型的query端加入可学习的软前缀，将这些高质量排序知识蒸馏进轻量embedding模型**，以此实现论文检索系统在**保证高吞吐量的同时显著提升检索精度**。

基于embedding技术的结果，我们又进一步将此技术用于学术影响力评估。**学术成果的影响力评估不仅依赖于论文的引用数量，还涉及引用语境、研究主题的相似性及跨领域的传播效果。**传统的基于静态引用计数的方法存在时效性差、缺乏语义理解等问题。

目前计算机领域没有动态更新的数据集，本项目收集计算机科学领域核心会议与期刊的论文元信息(如Title、abstract、authors等)，**构成计算机领域的超大规模数据集**，基于此数据集进行**动态影响力评估**。同时，**根据现有影响力算法TNCSISP，加入embedding进行相关论文检索改进，在本地库上进行精准计算。**本项目本地库来源：

- **CS领域自有论文集**：收集计算机科学领域核心会议与期刊的论文信息，构成本地库的主体。  
- **Scholar数据集**：下载并整理外部公开的 Scholar 数据，用于丰富本地库的引用关系与跨领域连接。  

### 1.2  研究意义

针对论文检索任务与数据集动态影响力评估，本项目提出以下创新性设计：

- **高质量伪标注构建**
  使用 Qwen3-Reranker-0.6B 为任意论文题目对打分，生成大规模高精度的相关性矩阵，作为训练数据。

- **Query端Soft Prompt精调**
  基于 Qwen3-Embedding-0.6B，仅在query侧拼接可学习soft tokens，将其作为“任务描述提示”进行优化，通过Bayesian Personalized Ranking (BPR) 排序损失训练，使embedding模型在匹配论文语义相关性时更加精准。

- **低成本高效部署**
  冻结embedding底座参数，仅训练极少量prompt参数，推理开销几乎不变，却能显著提升排序性能。

- **embedding与影响力评估结合**

  基于现有影响力评估算法TNCSISP，在本地库引入embedding，进行更高效的相似检索，从而更准确更快速的进行影响力评估。

- **动态更新数据集标签**

  定期抓取新论文及新引用，更新数据集论文元信息，重新计算影响力，使得影响力评估结果更具有时效性。

该方案可为学术论文推荐系统提供**接近reranker精度、接近embedding速度**的最佳平衡，显著改善科研人员的文献发现效率。同时，**结合embedding和影响力评估，创建动态更新数据集**，使评估结果更客观更准确更具有时效性。

### 1.3  当前主要挑战

1. **伪标注质量控制**
   reranker输出虽然精度高，但仍**存在一定噪声**。如果直接使用全部伪标注对进行训练，容易将噪声模式也学进去，引发模型偏置甚至误导。
   需要通过**得分阈值筛选、基于排名差距的样本选择、以及人工抽样验证**等方式提升训练数据的可靠性。

2. **Prompt 负优化风险**
   目前最大的困难是：Soft Prompt 训练后模型效果**明显劣于未训练状态**，即出现“负优化”现象。这通常源于：

   - 伪标注样本中正负对分布极不平衡；
   - Prompt 参数极少、梯度极易被主干网络压制；
   - 训练初期学习率不当，导致 Prompt 更新方向错误并放大噪声。

   因此，需要重点设计**prompt初始化策略（如用高质量示例文本初始化）、学习率warmup、损失平滑与梯度裁剪**等机制，确保训练不偏离正确优化方向。

3. **排序损失与样本构造**
   目前使用的Bayesian Personalized Ranking (BPR) 损失仅依赖相对顺序信息，对长尾数据和噪声非常敏感，可能强化伪标注误差。
   未来需要探索**Margin Loss、InfoNCE 等更稳健的排序损失**，以及“高置信度正样本 vs 随机负样本”的对比式样本构造方式，来提升训练信号质量。

4. **跨领域迁移与泛化**
   Soft Prompt 属于任务特定参数，跨领域泛化能力有限。当前在跨学科论文集合上的效果仍不稳定，容易退化。
   需要考虑**引入多领域混合数据、加入论文领域标签、或采用Contrastive Learning 对比学习**等方法增强模型鲁棒性和泛化性。

5. **本地库引入embedding**

   进行影响力评估前，需要进行相似论文检索，根据检索出来的论文的引用次数进行影响力评估。

   需要考虑如何使用embedding技术在本地库大规模数据中检索出相似度更高的论文，进行影响力计算。同时，还需考虑如何自动更新数据集标签，实现全自动更新论文影响力，使得结果更具有时效性。

***

## 二、学习资料与参考文献

- **BPR: Bayesian Personalized Ranking from Implicit Feedback (2009)**  
  提出:BPR 损失，用于基于隐式反馈的 pairwise 排序学习。

- **CoOp: Learning to Prompt for Vision-Language Models (2022)**  
  提出:在输入前拼接可学习 prompt token，通过少量参数提升跨模态模型性能。

- **Prefix-Tuning: Optimizing Continuous Prompts for Generation (2021)**  
  提出:通过在输入序列前加入可训练连续向量进行参数高效微调，仅训练极少参数。

- **The Power of Scale: Parameter-Efficient Adaptation for Pretrained Language Models (2021)**  
  系统化阐述多种参数高效调优（PEFT）方法，验证 prefix tuning、prompt tuning 在大模型上的有效性与可扩展性。

- **Introduction to Information Retrieval (2008)**  
  信息检索领域经典教材，系统介绍检索模型、排序算法与评价指标体系。

- **From Words to Worth: Newborn Article Impact Prediction with LLM（2024）**

  提出：在传统影响力算法TNCSI上加上“时间归一”，提出评估指标TNCSISP，使影响力具备跨学科跨年份可比性。

---

### 🧪 工程与工具

- **Qwen3系列** 模型主页  ：[QwenLM/Qwen3-Embedding](https://github.com/QwenLM/Qwen3-Embedding)  

***

## 三、结语与期望

本项目从 reranker 与 embedding 模型的互补性出发，提出了**利用高精度reranker生成大规模伪标注数据，训练query端Soft Prompt增强embedding模型在论文检索任务中的表现**这一新范式。同时，结合embedding模型与影响力评估，提出了**利用embedding更精准检索相似论文进行影响力评估的动态数据集**。

这种方法可以在保持embedding模型极高推理效率的同时，显著提升其语义匹配精度，为学术论文推荐系统带来**高效率、高精度、低成本**的综合优势。同时，也能使得影响力的计算结果**更准确、更具有参考性与时效性**。

未来的研究方向包括：

- 结合知识蒸馏，将reranker的深层交互信息迁移到embedding模型；
- 加入多模态信息（论文摘要、关键词、引用网络）进一步提升泛化性；
- 探索自监督或对比学习框架，降低对伪标注数据质量的依赖。

通过不断改进，本项目有望成为新一代论文检索与推荐系统的重要技术基石。
